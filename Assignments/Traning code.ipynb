{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQcVQ9_rde5P",
        "outputId": "f5ae369d-a799-460c-e96b-0800a2790544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Fold 1\n",
            "[0]\ttrain-rmse:24.71988\tvalid-rmse:24.93222\n",
            "[100]\ttrain-rmse:22.66098\tvalid-rmse:23.28225\n",
            "[200]\ttrain-rmse:21.84184\tvalid-rmse:22.83994\n",
            "[300]\ttrain-rmse:21.32428\tvalid-rmse:22.64229\n",
            "[400]\ttrain-rmse:20.94332\tvalid-rmse:22.54027\n",
            "[500]\ttrain-rmse:20.59936\tvalid-rmse:22.47052\n",
            "[600]\ttrain-rmse:20.27395\tvalid-rmse:22.42088\n",
            "[700]\ttrain-rmse:20.00177\tvalid-rmse:22.37906\n",
            "[800]\ttrain-rmse:19.73281\tvalid-rmse:22.35463\n",
            "[900]\ttrain-rmse:19.48731\tvalid-rmse:22.33267\n",
            "[1000]\ttrain-rmse:19.24639\tvalid-rmse:22.31992\n",
            "[1100]\ttrain-rmse:19.02005\tvalid-rmse:22.30254\n",
            "[1200]\ttrain-rmse:18.80832\tvalid-rmse:22.29848\n",
            "[1300]\ttrain-rmse:18.59787\tvalid-rmse:22.29437\n",
            "[1400]\ttrain-rmse:18.39955\tvalid-rmse:22.28865\n",
            "[1500]\ttrain-rmse:18.21450\tvalid-rmse:22.28816\n",
            "[1534]\ttrain-rmse:18.14826\tvalid-rmse:22.29044\n",
            "XGBoost Fold 2\n",
            "[0]\ttrain-rmse:24.86602\tvalid-rmse:24.33690\n",
            "[100]\ttrain-rmse:22.75989\tvalid-rmse:22.87231\n",
            "[200]\ttrain-rmse:21.93385\tvalid-rmse:22.49350\n",
            "[300]\ttrain-rmse:21.42177\tvalid-rmse:22.34347\n",
            "[400]\ttrain-rmse:21.02616\tvalid-rmse:22.25290\n",
            "[500]\ttrain-rmse:20.68353\tvalid-rmse:22.19936\n",
            "[600]\ttrain-rmse:20.36237\tvalid-rmse:22.16154\n",
            "[700]\ttrain-rmse:20.07380\tvalid-rmse:22.13587\n",
            "[800]\ttrain-rmse:19.79504\tvalid-rmse:22.11664\n",
            "[900]\ttrain-rmse:19.53767\tvalid-rmse:22.10223\n",
            "[1000]\ttrain-rmse:19.30882\tvalid-rmse:22.09377\n",
            "[1100]\ttrain-rmse:19.07724\tvalid-rmse:22.08048\n",
            "[1200]\ttrain-rmse:18.87069\tvalid-rmse:22.07612\n",
            "[1300]\ttrain-rmse:18.67130\tvalid-rmse:22.07215\n",
            "[1400]\ttrain-rmse:18.46530\tvalid-rmse:22.06980\n",
            "[1428]\ttrain-rmse:18.40652\tvalid-rmse:22.07202\n",
            "XGBoost Fold 3\n",
            "[0]\ttrain-rmse:24.89948\tvalid-rmse:24.20423\n",
            "[100]\ttrain-rmse:22.84063\tvalid-rmse:22.52664\n",
            "[200]\ttrain-rmse:22.00467\tvalid-rmse:22.08531\n",
            "[300]\ttrain-rmse:21.46523\tvalid-rmse:21.91742\n",
            "[400]\ttrain-rmse:21.06707\tvalid-rmse:21.82278\n",
            "[500]\ttrain-rmse:20.71571\tvalid-rmse:21.76533\n",
            "[600]\ttrain-rmse:20.39834\tvalid-rmse:21.71857\n",
            "[700]\ttrain-rmse:20.11125\tvalid-rmse:21.68498\n",
            "[800]\ttrain-rmse:19.85071\tvalid-rmse:21.66658\n",
            "[900]\ttrain-rmse:19.59228\tvalid-rmse:21.65507\n",
            "[1000]\ttrain-rmse:19.35412\tvalid-rmse:21.64235\n",
            "[1100]\ttrain-rmse:19.12235\tvalid-rmse:21.63113\n",
            "[1200]\ttrain-rmse:18.89672\tvalid-rmse:21.61751\n",
            "[1300]\ttrain-rmse:18.68741\tvalid-rmse:21.61661\n",
            "[1361]\ttrain-rmse:18.56465\tvalid-rmse:21.61726\n",
            "XGBoost Fold 4\n",
            "[0]\ttrain-rmse:24.55414\tvalid-rmse:25.58127\n",
            "[100]\ttrain-rmse:22.51741\tvalid-rmse:23.90796\n",
            "[200]\ttrain-rmse:21.69076\tvalid-rmse:23.42865\n",
            "[300]\ttrain-rmse:21.17747\tvalid-rmse:23.22010\n",
            "[400]\ttrain-rmse:20.79170\tvalid-rmse:23.12643\n",
            "[500]\ttrain-rmse:20.47405\tvalid-rmse:23.07403\n",
            "[600]\ttrain-rmse:20.15946\tvalid-rmse:23.03176\n",
            "[700]\ttrain-rmse:19.87077\tvalid-rmse:22.99968\n",
            "[800]\ttrain-rmse:19.61309\tvalid-rmse:22.97730\n",
            "[900]\ttrain-rmse:19.34624\tvalid-rmse:22.96534\n",
            "[1000]\ttrain-rmse:19.11845\tvalid-rmse:22.95630\n",
            "[1100]\ttrain-rmse:18.89642\tvalid-rmse:22.94609\n",
            "[1200]\ttrain-rmse:18.68176\tvalid-rmse:22.94256\n",
            "[1300]\ttrain-rmse:18.48770\tvalid-rmse:22.93908\n",
            "[1400]\ttrain-rmse:18.28937\tvalid-rmse:22.92797\n",
            "[1500]\ttrain-rmse:18.10352\tvalid-rmse:22.93085\n",
            "[1504]\ttrain-rmse:18.09677\tvalid-rmse:22.93069\n",
            "XGBoost Fold 5\n",
            "[0]\ttrain-rmse:24.76214\tvalid-rmse:24.75690\n",
            "[100]\ttrain-rmse:22.69600\tvalid-rmse:23.20346\n",
            "[200]\ttrain-rmse:21.88152\tvalid-rmse:22.73733\n",
            "[300]\ttrain-rmse:21.37029\tvalid-rmse:22.54152\n",
            "[400]\ttrain-rmse:20.96172\tvalid-rmse:22.42532\n",
            "[500]\ttrain-rmse:20.62941\tvalid-rmse:22.36068\n",
            "[600]\ttrain-rmse:20.31290\tvalid-rmse:22.31409\n",
            "[700]\ttrain-rmse:20.03353\tvalid-rmse:22.29028\n",
            "[800]\ttrain-rmse:19.78970\tvalid-rmse:22.27017\n",
            "[900]\ttrain-rmse:19.52412\tvalid-rmse:22.24807\n",
            "[1000]\ttrain-rmse:19.29736\tvalid-rmse:22.24278\n",
            "[1100]\ttrain-rmse:19.08354\tvalid-rmse:22.23351\n",
            "[1200]\ttrain-rmse:18.87336\tvalid-rmse:22.22473\n",
            "[1300]\ttrain-rmse:18.66180\tvalid-rmse:22.22198\n",
            "[1400]\ttrain-rmse:18.46253\tvalid-rmse:22.21454\n",
            "[1500]\ttrain-rmse:18.28997\tvalid-rmse:22.21737\n",
            "[1600]\ttrain-rmse:18.09118\tvalid-rmse:22.21231\n",
            "[1700]\ttrain-rmse:17.90193\tvalid-rmse:22.21128\n",
            "[1800]\ttrain-rmse:17.72135\tvalid-rmse:22.21106\n",
            "[1834]\ttrain-rmse:17.65784\tvalid-rmse:22.21130\n",
            "LightGBM Fold 1\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010288 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 882\n",
            "[LightGBM] [Info] Number of data points in the train set: 23040, number of used features: 143\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 23.166184\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1055]\ttrain's rmse: 19.1868\tvalid's rmse: 22.4007\n",
            "LightGBM Fold 2\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010851 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 886\n",
            "[LightGBM] [Info] Number of data points in the train set: 23040, number of used features: 145\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 23.430017\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[869]\ttrain's rmse: 19.5862\tvalid's rmse: 22.159\n",
            "LightGBM Fold 3\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010091 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 886\n",
            "[LightGBM] [Info] Number of data points in the train set: 23040, number of used features: 145\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 23.261139\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1039]\ttrain's rmse: 19.3205\tvalid's rmse: 21.6828\n",
            "LightGBM Fold 4\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013403 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 884\n",
            "[LightGBM] [Info] Number of data points in the train set: 23040, number of used features: 144\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 23.102522\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[833]\ttrain's rmse: 19.5184\tvalid's rmse: 23.031\n",
            "LightGBM Fold 5\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010126 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 882\n",
            "[LightGBM] [Info] Number of data points in the train set: 23040, number of used features: 143\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 23.228529\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1338]\ttrain's rmse: 18.6856\tvalid's rmse: 22.3664\n",
            "XGBoost OOF RMSE: 22.2284\n",
            "LightGBM OOF RMSE: 22.3322\n",
            "Ensemble OOF RMSE: 22.2473\n",
            "Validation C-Index (Kaggle Metric): 0.6084\n",
            "Submission file saved.\n",
            "Training complete. Models saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from lifelines.utils import concordance_index\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==============================\n",
        "# Data Loading and Preprocessing\n",
        "# ==============================\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv('/content/train.csv', index_col='ID')\n",
        "test = pd.read_csv('/content/test.csv', index_col='ID')\n",
        "sub = pd.read_csv('/content/sample_submission.csv', index_col='ID')\n",
        "\n",
        "# One-hot encode categorical features\n",
        "train = pd.get_dummies(train, drop_first=True)\n",
        "test = pd.get_dummies(test, drop_first=True)\n",
        "\n",
        "# Align train and test sets\n",
        "train, test = train.align(test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Replace problematic column names\n",
        "train.columns = train.columns.str.replace('[\\\\[\\\\]<>,]', '_', regex=True)\n",
        "test.columns = test.columns.str.replace('[\\\\[\\\\]<>,]', '_', regex=True)\n",
        "\n",
        "# Drop unnecessary columns from test set\n",
        "drop_cols = ['efs', 'efs_time', 'naf_label']\n",
        "test = test.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "# ====================\n",
        "# XGBoost Training\n",
        "# ====================\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'n_estimators': 2500,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "xgb_predictions = np.zeros(test.shape[0])\n",
        "xgb_oof = np.zeros(train.shape[0])\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # âœ… Proper instantiation\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(train.drop(columns=drop_cols, errors='ignore'))):\n",
        "    print(f\"XGBoost Fold {fold + 1}\")\n",
        "\n",
        "    X_train = train.iloc[train_idx].drop(columns=drop_cols, errors='ignore')\n",
        "    X_valid = train.iloc[valid_idx].drop(columns=drop_cols, errors='ignore')\n",
        "    y_train = train.iloc[train_idx]['efs_time']\n",
        "    y_valid = train.iloc[valid_idx]['efs_time']\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "    dtest = xgb.DMatrix(test)\n",
        "\n",
        "    model = xgb.train(\n",
        "        xgb_params, dtrain, num_boost_round=2500,\n",
        "        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose_eval=100\n",
        "    )\n",
        "\n",
        "    xgb_oof[valid_idx] = model.predict(dvalid)\n",
        "    xgb_predictions += model.predict(dtest) / kf.n_splits\n",
        "\n",
        "# ==============\n",
        "# LightGBM Model\n",
        "# ==============\n",
        "\n",
        "lgb_params = {\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': -1,\n",
        "    'num_leaves': 40,\n",
        "    'subsample': 0.9,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'lambda_l1': 2,\n",
        "    'lambda_l2': 5,\n",
        "    'n_estimators': 2500,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "lgb_predictions = np.zeros(test.shape[0])\n",
        "lgb_oof = np.zeros(train.shape[0])\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(train.drop(columns=drop_cols, errors='ignore'))):\n",
        "    print(f\"LightGBM Fold {fold + 1}\")\n",
        "\n",
        "    X_train = train.iloc[train_idx].drop(columns=drop_cols, errors='ignore')\n",
        "    X_valid = train.iloc[valid_idx].drop(columns=drop_cols, errors='ignore')\n",
        "    y_train = train.iloc[train_idx]['efs_time']\n",
        "    y_valid = train.iloc[valid_idx]['efs_time']\n",
        "\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    dvalid = lgb.Dataset(X_valid, label=y_valid, reference=dtrain)\n",
        "\n",
        "    model = lgb.train(\n",
        "        lgb_params,\n",
        "        dtrain,\n",
        "        num_boost_round=2500,\n",
        "        valid_sets=[dtrain, dvalid],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[lgb.early_stopping(100)]\n",
        "    )\n",
        "\n",
        "    lgb_oof[valid_idx] = model.predict(X_valid)\n",
        "    lgb_predictions += model.predict(test) / kf.n_splits\n",
        "\n",
        "# ==================\n",
        "# Model Evaluation\n",
        "# ==================\n",
        "\n",
        "# Define RMSE function\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Compute RMSE for OOF predictions\n",
        "xgb_rmse = rmse(train['efs_time'], xgb_oof)\n",
        "lgb_rmse = rmse(train['efs_time'], lgb_oof)\n",
        "ensemble_oof = (xgb_oof + lgb_oof) / 2\n",
        "ensemble_rmse = rmse(train['efs_time'], ensemble_oof)\n",
        "\n",
        "print(f\"XGBoost OOF RMSE: {xgb_rmse:.4f}\")\n",
        "print(f\"LightGBM OOF RMSE: {lgb_rmse:.4f}\")\n",
        "print(f\"Ensemble OOF RMSE: {ensemble_rmse:.4f}\")\n",
        "\n",
        "# Compute C-Index (Main Kaggle Metric)\n",
        "c_index = concordance_index(train['efs_time'], ensemble_oof, event_observed=train['efs'])\n",
        "print(f\"Validation C-Index (Kaggle Metric): {c_index:.4f}\")\n",
        "\n",
        "# ========================\n",
        "# Ensemble and Submission\n",
        "# ========================\n",
        "\n",
        "# Simple average ensemble\n",
        "ensemble_predictions = (xgb_predictions + lgb_predictions) / 2\n",
        "\n",
        "# Save predictions\n",
        "sub['prediction'] = ensemble_predictions\n",
        "sub.to_csv('submission.csv')\n",
        "print(\"Submission file saved.\")\n",
        "\n",
        "# Save models\n",
        "joblib.dump(ensemble_rmse, \"ensemble_models.pkl\")\n",
        "print(\"Training complete. Models saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXhplpH_b-vj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}